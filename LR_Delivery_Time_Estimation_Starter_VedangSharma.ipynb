{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8uOZe3UO68U-"
   },
   "source": [
    "# Order Delivery Time Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qd9e2-HF6_85"
   },
   "source": [
    "## Objectives\n",
    "The objective of this assignment is to build a regression model that predicts the delivery time for orders placed through Porter. The model will use various features such as the items ordered, the restaurant location, the order protocol, and the availability of delivery partners.\n",
    "\n",
    "The key goals are:\n",
    "- Predict the delivery time for an order based on multiple input features\n",
    "- Improve delivery time predictions to optimiae operational efficiency\n",
    "- Understand the key factors influencing delivery time to enhance the model's accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcC6tJ2p7F2p"
   },
   "source": [
    "## Data Pipeline\n",
    "The data pipeline for this assignment will involve the following steps:\n",
    "1. **Data Loading**\n",
    "2. **Data Preprocessing and Feature Engineering**\n",
    "3. **Exploratory Data Analysis**\n",
    "4. **Model Building**\n",
    "5. **Model Inference**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGOQI_f72jV1"
   },
   "source": [
    "## Data Understanding\n",
    "The dataset contains information on orders placed through Porter, with the following columns:\n",
    "\n",
    "| Field                     | Description                                                                                 |\n",
    "|---------------------------|---------------------------------------------------------------------------------------------|\n",
    "| market_id                 | Integer ID representing the market where the restaurant is located.                         |\n",
    "| created_at                | Timestamp when the order was placed.                                                        |\n",
    "| actual_delivery_time      | Timestamp when the order was delivered.                                                     |\n",
    "| store_primary_category    | Category of the restaurant (e.g., fast food, dine-in).                                      |\n",
    "| order_protocol            | Integer representing how the order was placed (e.g., via Porter, call to restaurant, etc.). |\n",
    "| total_items               | Total number of items in the order.                                                         |\n",
    "| subtotal                  | Final price of the order.                                                                   |\n",
    "| num_distinct_items        | Number of distinct items in the order.                                                      |\n",
    "| min_item_price            | Price of the cheapest item in the order.                                                    |\n",
    "| max_item_price            | Price of the most expensive item in the order.                                              |\n",
    "| total_onshift_dashers     | Number of delivery partners on duty when the order was placed.                              |\n",
    "| total_busy_dashers        | Number of delivery partners already occupied with other orders.                             |\n",
    "| total_outstanding_orders  | Number of orders pending fulfillment at the time of the order.                              |\n",
    "| distance                  | Total distance from the restaurant to the customer.                                         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5QoCQFDzHUWP"
   },
   "source": [
    "## **Importing Necessary Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jun9CeAc7QOw"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MueJxkvUIII3"
   },
   "source": [
    "## **1. Loading the data**\n",
    "Load 'porter_data_1.csv' as a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VJS8ZRJXHTwv"
   },
   "outputs": [],
   "source": [
    "# Importing the file porter_data_1.csv\n",
    "df = pd.read_csv('porter_data_1.csv')\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSRQocOkMSQl"
   },
   "source": [
    "## **2. Data Preprocessing and Feature Engineering** <font color = red>[15 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02uPO8aQfLnn"
   },
   "source": [
    "#### **2.1 Fixing the Datatypes**  <font color = red>[5 marks]</font> <br>\n",
    "The current timestamps are in object format and need conversion to datetime format for easier handling and intended functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b22Kzjew3rdM"
   },
   "source": [
    "##### **2.1.1** <font color = red>[2 marks]</font> <br>\n",
    "Convert date and time fields to appropriate data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FoGkz909IXjv"
   },
   "outputs": [],
   "source": [
    "# Convert 'created_at' and 'actual_delivery_time' columns to datetime format\n",
    "\n",
    "# Convert categorical features to category type\n",
    "df['market_id'] = df['market_id'].astype('category')\n",
    "df['store_primary_category'] = df['store_primary_category'].astype('category')\n",
    "df['order_protocol'] = df['order_protocol'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u1EBPjFc4Qca"
   },
   "source": [
    "##### **2.1.2**  <font color = red>[3 marks]</font> <br>\n",
    "Convert categorical fields to appropriate data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PihPSPhQq1nQ"
   },
   "outputs": [],
   "source": [
    "# Convert categorical features to category type\n",
    "\n",
    "# Convert categorical features to category type\n",
    "df['market_id'] = df['market_id'].astype('category')\n",
    "df['store_primary_category'] = df['store_primary_category'].astype('category')\n",
    "df['order_protocol'] = df['order_protocol'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hsEGroRFlX8z"
   },
   "source": [
    "#### **2.2 Feature Engineering** <font color = red>[5 marks]</font> <br>\n",
    "Calculate the time taken to execute the delivery as well as extract the hour and day at which the order was placed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BubGzQyJpHLQ"
   },
   "source": [
    "##### **2.2.1** <font color = red>[2 marks]</font> <br>\n",
    "Calculate the time taken using the features `actual_delivery_time` and `created_at`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uBGS4PZJMciZ"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Calculate time taken in minutes\n",
    "df['time_taken'] = (df['actual_delivery_time'] - df['created_at']).dt.total_seconds() / 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngUUAf3XOPAP"
   },
   "source": [
    "##### **2.2.2** <font color = red>[3 marks]</font> <br>\n",
    "Extract the hour at which the order was placed and which day of the week it was. Drop the unnecessary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iwA4O5VtNxQW"
   },
   "outputs": [],
   "source": [
    "# Extract the hour and day of week from the 'created_at' timestamp\n",
    "\n",
    "# Extract the hour and day of week from the 'created_at' timestamp\n",
    "df['hour'] = df['created_at'].dt.hour\n",
    "df['day_of_week'] = df['created_at'].dt.day_name()\n",
    "\n",
    "# Create a categorical feature 'isWeekend'\n",
    "df['is_weekend'] = df['day_of_week'].isin(['Saturday', 'Sunday']).astype(int)\n",
    "\n",
    "\n",
    "# Create a categorical feature 'isWeekend'\n",
    "\n",
    "# Extract the hour and day of week from the 'created_at' timestamp\n",
    "df['hour'] = df['created_at'].dt.hour\n",
    "df['day_of_week'] = df['created_at'].dt.day_name()\n",
    "\n",
    "# Create a categorical feature 'isWeekend'\n",
    "df['is_weekend'] = df['day_of_week'].isin(['Saturday', 'Sunday']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZgzSO8wyOTbP"
   },
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "df = df.drop(columns=['created_at', 'actual_delivery_time', 'day_of_week'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-JJxTsQOFKyl"
   },
   "source": [
    "#### **2.3 Creating training and validation sets** <font color = red>[5 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wuyPJMpCFyUL"
   },
   "source": [
    "##### **2.3.1** <font color = red>[2 marks]</font> <br>\n",
    " Define target and input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BVyKFLXTFKRE"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define target variable (y) and features (X)\n",
    "y = df['time_taken']\n",
    "X = df.drop(columns=['time_taken'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e56iVNqdF3G8"
   },
   "source": [
    "##### **2.3.2** <font color = red>[3 marks]</font> <br>\n",
    " Split the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0t7XtNDEF6Pu"
   },
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQxv96NBAq_y"
   },
   "source": [
    "## **3. Exploratory Data Analysis on Training Data** <font color = red>[20 marks]</font> <br>\n",
    "1. Analyzing the correlation between variables to identify patterns and relationships\n",
    "2. Identifying and addressing outliers to ensure the integrity of the analysis\n",
    "3. Exploring the relationships between variables and examining the distribution of the data for better insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VU1baEcRc1-A"
   },
   "source": [
    "#### **3.1 Feature Distributions** <font color = red> [7 marks]</font> <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rj7yFI7VJ_va"
   },
   "outputs": [],
   "source": [
    "# Define numerical and categorical columns for easy EDA and data manipulation\n",
    "# Define numerical and categorical columns for easy EDA and data manipulation\n",
    "numerical_cols = X_train.select_dtypes(include=['number']).columns.tolist()\n",
    "categorical_cols = X_train.select_dtypes(include=['category', 'object']).columns.tolist()\n",
    "\n",
    "print(\"Numerical Columns:\", numerical_cols)\n",
    "print(\"Categorical Columns:\", categorical_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fWMFLWKpHE-R"
   },
   "source": [
    "##### **3.1.1** <font color = red>[3 marks]</font> <br>\n",
    "Plot distributions for numerical columns in the training set to understand their spread and any skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_M0u5G1YR73_"
   },
   "outputs": [],
   "source": [
    "# Plot distributions for all numerical columns\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot distributions for all numerical columns\n",
    "plt.figure(figsize=(15, 20))\n",
    "for i, col in enumerate(numerical_cols):\n",
    "    plt.subplot(5, 4, i + 1)  # Assuming a 5x4 grid is sufficient\n",
    "    sns.histplot(X_train[col], kde=True) # Plot histogram with KDE\n",
    "    plt.title(col)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4MtpapIvc9rC"
   },
   "source": [
    "##### **3.1.2** <font color = red>[2 marks]</font> <br>\n",
    "Check the distribution of categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zr8loNgMLdrm"
   },
   "outputs": [],
   "source": [
    "# Distribution of categorical columns\n",
    "\n",
    "# Distribution of categorical columns\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(categorical_cols):\n",
    "    plt.subplot(2, 2, i + 1) # Assuming a 2x2 grid is sufficient\n",
    "    sns.countplot(data=X_train, x=col)\n",
    "    plt.title(col)\n",
    "    plt.xticks(rotation=45) # Rotate x-axis labels for readability\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-9pcLxzJZWf"
   },
   "source": [
    "##### **3.1.3** <font color = red>[2 mark]</font> <br>\n",
    "Visualise the distribution of the target variable to understand its spread and any skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fiWe2Bl9R7yL"
   },
   "outputs": [],
   "source": [
    "# Distribution of time_taken\n",
    "\n",
    "# Distribution of time_taken\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(y_train, kde=True)\n",
    "plt.title('Distribution of time_taken')\n",
    "plt.xlabel('Time Taken (minutes)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbxczs61dROZ"
   },
   "source": [
    "#### **3.2 Relationships Between Features** <font color = red>[3 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YH81kNkOOvlx"
   },
   "source": [
    "##### **3.2.1** <font color = red>[3 marks]</font> <br>\n",
    "Scatter plots for important numerical and categorical features to observe how they relate to `time_taken`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zIBnRHohR799"
   },
   "outputs": [],
   "source": [
    "# Scatter plot to visualise the relationship between time_taken and other features\n",
    "\n",
    "# Scatter plot to visualise the relationship between time_taken and other numerical features\n",
    "plt.figure(figsize=(15, 20))\n",
    "for i, col in enumerate(numerical_cols):\n",
    "    if col != 'time_taken':  # Avoid plotting time_taken against itself\n",
    "        plt.subplot(5, 4, i + 1)\n",
    "        sns.scatterplot(data=df, x=col, y='time_taken')\n",
    "        plt.title(f'{col} vs. time_taken')\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('time_taken')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KiWL3cKowfZd"
   },
   "outputs": [],
   "source": [
    "# Show the distribution of time_taken for different hours\n",
    "\n",
    "# Show the distribution of time_taken for different hours\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=df, x='hour', y='time_taken')\n",
    "plt.title('Distribution of Delivery Time by Hour of Day')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Delivery Time (minutes)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GKg6rBljIJFP"
   },
   "source": [
    "#### **3.3 Correlation Analysis** <font color = red>[5 marks]</font> <br>\n",
    "Check correlations between numerical features to identify which variables are strongly related to `time_taken`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cyk00sbYfnc0"
   },
   "source": [
    "##### **3.3.1** <font color = red>[3 marks]</font> <br>\n",
    "Plot a heatmap to display correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WxrdHdvKR7vy"
   },
   "outputs": [],
   "source": [
    "# Plot the heatmap of the correlation matrix\n",
    "\n",
    "# Check correlations between numerical features to identify which variables are strongly related to `time_taken`\n",
    "# Plot the heatmap of the correlation matrix\n",
    "\n",
    "numerical_cols_with_target = numerical_cols + ['time_taken'] # Include target variable\n",
    "correlation_matrix = df[numerical_cols_with_target].corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap of Numerical Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8yuD3RIwffZE"
   },
   "source": [
    "##### **3.3.2** <font color = red>[2 marks]</font> <br>\n",
    "Drop the columns with weak correlations with the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MDZN586gH8R_"
   },
   "outputs": [],
   "source": [
    "# Drop 3-5 weakly correlated columns from training dataset\n",
    "\n",
    "# Drop 3-5 weakly correlated columns from training dataset\n",
    "\n",
    "# 1. Identify weakly correlated columns\n",
    "weak_correlation_threshold = 0.1  # You can adjust this threshold\n",
    "columns_to_drop = []\n",
    "for col in numerical_cols:\n",
    "    if col != 'time_taken' and abs(correlation_matrix['time_taken'][col]) < weak_correlation_threshold:\n",
    "        columns_to_drop.append(col)\n",
    "\n",
    "print(\"Columns to drop:\", columns_to_drop)\n",
    "\n",
    "# 2. Drop the columns from the training set\n",
    "X_train_filtered = X_train.drop(columns=columns_to_drop)\n",
    "X_test_filtered = X_test.drop(columns=columns_to_drop)  # Also drop from the test set!\n",
    "\n",
    "# 3. Update numerical_cols (important for later)\n",
    "numerical_cols = [col for col in numerical_cols if col not in columns_to_drop]\n",
    "\n",
    "print(\"Updated Numerical Columns:\", numerical_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_mZv2rz6lxvc"
   },
   "source": [
    "#### **3.4 Handling the Outliers** <font color = red>[5 marks]</font> <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdyAT-OhyH3z"
   },
   "source": [
    "##### **3.4.1** <font color = red>[2 marks]</font> <br>\n",
    "Visualise potential outliers for the target variable and other numerical features using boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ow3Mowo4R71T"
   },
   "outputs": [],
   "source": [
    "# Boxplot for time_taken\n",
    "\n",
    "# Visualise potential outliers for the target variable and other numerical features using boxplots\n",
    "# Boxplot for time_taken\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(y=X_train['time_taken']) # Use y-axis for single variable boxplot\n",
    "plt.title('Boxplot of time_taken')\n",
    "plt.ylabel('Delivery Time (minutes)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZCaGBKv_stm"
   },
   "source": [
    "##### **3.4.2** <font color = red>[3 marks]</font> <br>\n",
    "Handle outliers present in all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cwQ1A_wZ_X_K"
   },
   "outputs": [],
   "source": [
    "# Handle outliers\n",
    "\n",
    "# Visualize potential outliers for all numerical features using boxplots\n",
    "plt.figure(figsize=(15, 20))\n",
    "for i, col in enumerate(numerical_cols):\n",
    "    plt.subplot(5, 4, i + 1)\n",
    "    sns.boxplot(y=X_train[col])\n",
    "    plt.title(f'Boxplot of {col}')\n",
    "    plt.ylabel(col)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "Q1 = X_train['time_taken'].quantile(0.25)\n",
    "Q3 = X_train['time_taken'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "X_train_filtered = X_train[(X_train['time_taken'] >= lower_bound) & (X_train['time_taken'] <= upper_bound)]\n",
    "y_train_filtered = y_train[(X_train['time_taken'] >= lower_bound) & (X_train['time_taken'] <= upper_bound)]\n",
    "\n",
    "#Remember to apply the same logic to the test set if needed\n",
    "X_test_filtered = X_test[(X_test['time_taken'] >= lower_bound) & (X_test['time_taken'] <= upper_bound)]\n",
    "y_test_filtered = y_test[(X_test['time_taken'] >= lower_bound) & (X_test['time_taken'] <= upper_bound)]\n",
    "\n",
    "upper_limit = X_train['time_taken'].quantile(0.95)  # 95th percentile\n",
    "X_train['time_taken'] = np.where(X_train['time_taken'] > upper_limit, upper_limit, X_train['time_taken'])\n",
    "X_test['time_taken'] = np.where(X_test['time_taken'] > upper_limit, upper_limit, X_test['time_taken'])\n",
    "\n",
    "upper_limit = X_train['time_taken'].quantile(0.95)  # 95th percentile\n",
    "X_train['time_taken'] = np.where(X_train['time_taken'] > upper_limit, upper_limit, X_train['time_taken'])\n",
    "X_test['time_taken'] = np.where(X_test['time_taken'] > upper_limit, upper_limit, X_test['time_taken'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0Cd2J-LGWaF"
   },
   "source": [
    "## **4. Exploratory Data Analysis on Validation Data** <font color = red>[optional]</font> <br>\n",
    "Optionally, perform EDA on test data to see if the distribution match with the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8sN6bG_hTbUE"
   },
   "outputs": [],
   "source": [
    "# Define numerical and categorical columns for easy EDA and data manipulation\n",
    "\n",
    "# Define numerical and categorical columns for easy EDA and data manipulation\n",
    "numerical_cols = X_train.select_dtypes(include=['number']).columns.tolist()\n",
    "categorical_cols = X_train.select_dtypes(include=['category', 'object']).columns.tolist()\n",
    "\n",
    "print(\"Numerical Columns:\", numerical_cols)\n",
    "print(\"Categorical Columns:\", categorical_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Zq16lr0Q9IG"
   },
   "source": [
    "#### **4.1 Feature Distributions**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WuoIVgXlQC9y"
   },
   "source": [
    "##### **4.1.1**\n",
    "Plot distributions for numerical columns in the validation set to understand their spread and any skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JKgSvKvzG8fv"
   },
   "outputs": [],
   "source": [
    "# Plot distributions for all numerical columns\n",
    "\n",
    "plt.figure(figsize=(15, 20))\n",
    "for i, col in enumerate(numerical_cols):\n",
    "    plt.subplot(5, 4, i + 1)\n",
    "    sns.histplot(X_test[col], kde=True)\n",
    "    plt.title(f'Test Data - {col} Distribution')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MrywBQGWQC9z"
   },
   "source": [
    "##### **4.1.2**\n",
    "Check the distribution of categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p0CIcl2tHBwp"
   },
   "outputs": [],
   "source": [
    "# Distribution of categorical columns\n",
    "\n",
    "# Distribution of categorical columns in Test Set\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(categorical_cols):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    sns.countplot(data=X_test, x=col)\n",
    "    plt.title(f'Test Data - {col} Distribution')\n",
    "    plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8_j74bnlQC9z"
   },
   "source": [
    "##### **4.1.3**\n",
    "Visualise the distribution of the target variable to understand its spread and any skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_dGfR8MHGtqm"
   },
   "outputs": [],
   "source": [
    "# Distribution of time_taken\n",
    "\n",
    "# Distribution of time_taken in Test Set\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(y_test, kde=True)\n",
    "plt.title('Distribution of time_taken in Test Set')\n",
    "plt.xlabel('Time Taken (minutes)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ki2FI7fsHDgK"
   },
   "source": [
    "#### **4.2 Relationships Between Features**\n",
    "Scatter plots for numerical features to observe how they relate to each other, especially to `time_taken`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8lzNPoK4HFnZ"
   },
   "outputs": [],
   "source": [
    "# Scatter plot to visualise the relationship between time_taken and other features\n",
    "\n",
    "# Scatter plot to visualise the relationship between time_taken and other numerical features in Test Set\n",
    "plt.figure(figsize=(15, 20))\n",
    "for i, col in enumerate(numerical_cols):\n",
    "    if col != 'time_taken':\n",
    "        plt.subplot(5, 4, i + 1)\n",
    "        sns.scatterplot(x=X_test[col], y=y_test)\n",
    "        plt.title(f'Test Data - {col} vs. time_taken')\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('time_taken')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8VoM0XfXWko"
   },
   "source": [
    "#### **4.3** Drop the columns with weak correlations with the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1BnM8w2lXWkp"
   },
   "outputs": [],
   "source": [
    "# Drop the weakly correlated columns from training dataset\n",
    "\n",
    "# 1. Identify weakly correlated columns\n",
    "weak_correlation_threshold = 0.1  # You can adjust this threshold\n",
    "columns_to_drop = []\n",
    "for col in numerical_cols:\n",
    "    if col != 'time_taken' and abs(correlation_matrix['time_taken'][col]) < weak_correlation_threshold:\n",
    "        columns_to_drop.append(col)\n",
    "\n",
    "print(\"Columns to drop:\", columns_to_drop)\n",
    "\n",
    "# 2. Drop the columns from the training set\n",
    "X_train_filtered = X_train.drop(columns=columns_to_drop)\n",
    "X_test_filtered = X_test.drop(columns=columns_to_drop)  # Also drop from the test set!\n",
    "\n",
    "# 3. Update numerical_cols (important for later)\n",
    "numerical_cols = [col for col in numerical_cols if col not in columns_to_drop]\n",
    "\n",
    "print(\"Updated Numerical Columns:\", numerical_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ReNN4PyM8enl"
   },
   "source": [
    "## **5. Model Building** <font color = red>[15 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2l2XfNF6nc8L"
   },
   "source": [
    "#### **Import Necessary Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "__fmfT6vQWpd"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np # for numerical operations\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCLIKw5pQiA7"
   },
   "source": [
    "#### **5.1 Feature Scaling** <font color = red>[3 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "newEgSyyQiHK"
   },
   "outputs": [],
   "source": [
    "# Apply scaling to the numerical columns\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n",
    "X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])  # Use 'transform' on test data!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXcV5Z_E8tLL"
   },
   "source": [
    "Note that linear regression is agnostic to feature scaling. However, with feature scaling, we get the coefficients to be somewhat on the same scale so that it becomes easier to compare them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bxip-t3Y1MB"
   },
   "source": [
    "#### **5.2 Build a linear regression model** <font color = red>[5 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7jZciTFtric"
   },
   "source": [
    "You can choose from the libraries *statsmodels* and *scikit-learn* to build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DMRpgx_iQYM4"
   },
   "outputs": [],
   "source": [
    "# Create/Initialise the model\n",
    "\n",
    "model_sklearn = LinearRegression()\n",
    "\n",
    "# Add a constant term to the features matrix\n",
    "X_train_sm = sm.add_constant(X_train)\n",
    "\n",
    "# Create/Initialise the Linear Regression model\n",
    "model_statsmodels = sm.OLS(y_train, X_train_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hbJVZpMiW8b2"
   },
   "outputs": [],
   "source": [
    "# Train the model using the training data\n",
    "model_sklearn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cCQcJtDbW_dG"
   },
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_train = model_sklearn.predict(X_train)\n",
    "y_pred_test = model_sklearn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Udw5kE1fXBsR"
   },
   "outputs": [],
   "source": [
    "# Find results for evaluation metrics\n",
    "# Find results for evaluation metrics\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE)\n",
    "mae_train = mean_absolute_error(y_train, y_pred_train)\n",
    "mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "\n",
    "# Calculate R-squared (R2)\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "\n",
    "# Print the results\n",
    "print(\"Training Set Metrics:\")\n",
    "print(f\"MSE: {mse_train:.2f}\")\n",
    "print(f\"MAE: {mae_train:.2f}\")\n",
    "print(f\"R-squared: {r2_train:.2f}\")\n",
    "\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "print(f\"MSE: {mse_test:.2f}\")\n",
    "print(f\"MAE: {mae_test:.2f}\")\n",
    "print(f\"R-squared: {r2_test:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3-HovybcZKR"
   },
   "source": [
    "Note that we have 12 (depending on how you select features) training features. However, not all of them would be useful. Let's say we want to take the most relevant 8 features.\n",
    "\n",
    "We will use Recursive Feature Elimination (RFE) here.\n",
    "\n",
    "For this, you can look at the coefficients / p-values of features from the model summary and perform feature elimination, or you can use the RFE module provided with *scikit-learn*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zU8OLQ4bnwdr"
   },
   "source": [
    "#### **5.3 Build the model and fit RFE to select the most important features** <font color = red>[7 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4FZMiX11RyI"
   },
   "source": [
    "For RFE, we will start with all features and use\n",
    "the RFE method to recursively reduce the number of features one-by-one.\n",
    "\n",
    "After analysing the results of these iterations, we select the one that has a good balance between performance and number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ub1HgSwl1eiC"
   },
   "outputs": [],
   "source": [
    "# Loop through the number of features and test the model\n",
    "\n",
    "# Loop through the number of features and test the model\n",
    "mse_train_list = []\n",
    "mse_test_list = []\n",
    "mae_train_list = []\n",
    "mae_test_list = []\n",
    "r2_train_list = []\n",
    "r2_test_list = []\n",
    "\n",
    "num_features_to_test = [5, 10, 15, 20, len(numerical_cols) + len(categorical_cols)]  # You can adjust this list\n",
    "\n",
    "# Get feature importance from the model (if available)\n",
    "if hasattr(model_sklearn, 'coef_'):  # LinearRegression has 'coef_'\n",
    "    feature_importance = np.abs(model_sklearn.coef_)\n",
    "else:\n",
    "    # For models without coef_, you might need a different approach\n",
    "    print(\"Model doesn't provide feature coefficients for ranking.\")\n",
    "    feature_importance = np.zeros(X_train.shape[1])  # Placeholder\n",
    "\n",
    "# Sort features by importance\n",
    "feature_ranking = np.argsort(feature_importance)[::-1]  # Descending order\n",
    "\n",
    "for num_features in num_features_to_test:\n",
    "    # Select top features\n",
    "    top_features = X_train.columns[feature_ranking[:num_features]]\n",
    "\n",
    "    # Select features for training and testing\n",
    "    X_train_selected = X_train[top_features]\n",
    "    X_test_selected = X_test[top_features]\n",
    "\n",
    "    # Train the model\n",
    "    model = LinearRegression()  # Create a new model for each feature set\n",
    "    model.fit(X_train_selected, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred_train = model.predict(X_train_selected)\n",
    "    y_pred_test = model.predict(X_test_selected)\n",
    "\n",
    "    # Evaluate the model\n",
    "    mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "    mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "    mae_train = mean_absolute_error(y_train, y_pred_train)\n",
    "    mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "    r2_train = r2_score(y_train, y_pred_train)\n",
    "    r2_test = r2_score(y_test, y_pred_test)\n",
    "\n",
    "    # Store the results\n",
    "    mse_train_list.append(mse_train)\n",
    "    mse_test_list.append(mse_test)\n",
    "    mae_train_list.append(mae_train)\n",
    "    mae_test_list.append(mae_test)\n",
    "    r2_train_list.append(r2_train)\n",
    "    r2_test_list.append(r2_test)\n",
    "\n",
    "# Print the results\n",
    "print(\"Results for different number of features:\")\n",
    "for i, num_features in enumerate(num_features_to_test):\n",
    "    print(f\"\\nNumber of Features: {num_features}\")\n",
    "    print(f\"  Training Set - MSE: {mse_train_list[i] :.2f}, MAE: {mae_train_list[i] :.2f}, R-squared: {r2_train_list[i] :.2f}\")\n",
    "    print(f\"  Test Set     - MSE: {mse_test_list[i] :.2f}, MAE: {mae_test_list[i] :.2f}, R-squared: {r2_test_list[i] :.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M7p-CAQn3wQE"
   },
   "outputs": [],
   "source": [
    "# Build the final model with selected number of features\n",
    "\n",
    "# Build the final model with selected number of features\n",
    "\n",
    "# Determine the optimal number of features (you should replace this with your findings)\n",
    "optimal_num_features = 15  # Replace with the best number of features from your previous analysis\n",
    "\n",
    "# Select the top features based on the optimal number\n",
    "top_features = X_train.columns[feature_ranking[:optimal_num_features]]\n",
    "X_train_final = X_train[top_features]\n",
    "X_test_final = X_test[top_features]\n",
    "\n",
    "# Train the final model\n",
    "final_model = LinearRegression()\n",
    "final_model.fit(X_train_final, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train_final = final_model.predict(X_train_final)\n",
    "y_pred_test_final = final_model.predict(X_test_final)\n",
    "\n",
    "# Evaluate the final model\n",
    "mse_train_final = mean_squared_error(y_train, y_pred_train_final)\n",
    "mse_test_final = mean_squared_error(y_test, y_pred_test_final)\n",
    "mae_train_final = mean_absolute_error(y_train, y_pred_train_final)\n",
    "mae_test_final = mean_absolute_error(y_test, y_pred_test_final)\n",
    "r2_train_final = r2_score(y_train, y_pred_train_final)\n",
    "r2_test_final = r2_score(y_test, y_pred_test_final)\n",
    "\n",
    "# Print the final model's performance\n",
    "print(\"Final Model Performance:\")\n",
    "print(f\"  Training Set - MSE: {mse_train_final:.2f}, MAE: {mae_train_final:.2f}, R-squared: {r2_train_final:.2f}\")\n",
    "print(f\"  Test Set     - MSE: {mse_test_final:.2f}, MAE: {mae_test_final:.2f}, R-squared: {r2_test_final:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0l_mLL_4OOl"
   },
   "source": [
    "## **6. Results and Inference** <font color = red>[5 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsPGaacJ71mt"
   },
   "source": [
    "#### **6.1 Perform Residual Analysis** <font color = red>[3 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lbj7O8rf7SZS"
   },
   "outputs": [],
   "source": [
    "# Perform residual analysis using plots like residuals vs predicted values, Q-Q plot and residual histogram\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats  # For the Q-Q plot\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = y_test - y_pred_test_final\n",
    "\n",
    "# 1. Residuals vs. Predicted Values Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=y_pred_test_final, y=residuals)\n",
    "plt.title('Residuals vs. Predicted Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.axhline(y=0, color='r', linestyle='--')  # Add a horizontal line at y=0\n",
    "plt.show()\n",
    "\n",
    "# 2. Q-Q Plot (Quantile-Quantile Plot)\n",
    "plt.figure(figsize=(10, 6))\n",
    "stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "plt.title('Q-Q Plot of Residuals')\n",
    "plt.show()\n",
    "\n",
    "# 3. Residual Histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(residuals, kde=True)  # Plot histogram with KDE\n",
    "plt.title('Histogram of Residuals')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aq4g9xPsu4T5"
   },
   "source": [
    "[Your inferences here:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S2-CiCId7_y9"
   },
   "source": [
    "#### **6.2 Perform Coefficient Analysis** <font color = red>[2 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y2koFJovu-cH"
   },
   "source": [
    "Perform coefficient analysis to find how changes in features affect the target.\n",
    "Also, the features were scaled, so interpret the scaled and unscaled coefficients to understand the impact of feature changes on delivery time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sr8EWhg_9QnI"
   },
   "outputs": [],
   "source": [
    "# Compare the scaled vs unscaled features used in the final model\n",
    "\n",
    "# Compare the scaled vs unscaled features used in the final model\n",
    "\n",
    "# Unscaled coefficients (from the original model - before feature selection)\n",
    "unscaled_coef = model_sklearn.coef_\n",
    "unscaled_intercept = model_sklearn.intercept_\n",
    "\n",
    "# Scaled coefficients (from the final model - after feature selection)\n",
    "scaled_coef = final_model.coef_\n",
    "scaled_intercept = final_model.intercept_\n",
    "\n",
    "# Feature names (original)\n",
    "original_feature_names = X_train.columns\n",
    "\n",
    "# Feature names (selected)\n",
    "selected_feature_names = X_train_final.columns\n",
    "\n",
    "# Create a DataFrame for comparison\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Original Feature': original_feature_names,\n",
    "    'Unscaled Coefficient': unscaled_coef\n",
    "})\n",
    "\n",
    "comparison_df['Selected'] = comparison_df['Original Feature'].isin(selected_feature_names)\n",
    "\n",
    "comparison_df['Scaled Coefficient'] = 0  # Initialize with 0\n",
    "comparison_df.loc[comparison_df['Selected'], 'Scaled Coefficient'] = scaled_coef\n",
    "\n",
    "print(\"Comparison of Scaled vs. Unscaled Coefficients:\\n\")\n",
    "print(comparison_df)\n",
    "\n",
    "print(f\"\\nUnscaled Intercept: {unscaled_intercept:.4f}\")\n",
    "print(f\"Scaled Intercept:   {scaled_intercept:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQ5VcQ2G-SOb"
   },
   "source": [
    "Additionally, we can analyse the effect of a unit change in a feature. In other words, because we have scaled the features, a unit change in the features will not translate directly to the model. Use scaled and unscaled coefficients to find how will a unit change in a feature affect the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dMHN7r-x-Lp5"
   },
   "outputs": [],
   "source": [
    "# Analyze the effect of a unit change in a feature, say 'total_items'\n",
    "\n",
    "\n",
    "feature_of_interest = 'total_items'\n",
    "\n",
    "# Find the unscaled coefficient for 'total_items'\n",
    "unscaled_coef_total_items = comparison_df.loc[comparison_df['Original Feature'] == feature_of_interest, 'Unscaled Coefficient'].values[0]\n",
    "\n",
    "# Find the mean and standard deviation of 'total_items' from the training data (used for scaling)\n",
    "mean_total_items = X_train['total_items'].mean()\n",
    "std_total_items = X_train['total_items'].std()\n",
    "\n",
    "# Calculate the effect of a one-unit change in 'total_items'\n",
    "effect_of_one_unit_change = unscaled_coef_total_items\n",
    "\n",
    "# Calculate the effect of a one-standard-deviation change in 'total_items'\n",
    "# This is essentially what the scaled coefficient represents\n",
    "effect_of_one_std_change = comparison_df.loc[comparison_df['Original Feature'] == feature_of_interest, 'Scaled Coefficient'].values[0]\n",
    "\n",
    "print(f\"Feature: {feature_of_interest}\\n\")\n",
    "print(f\"Unscaled Coefficient: {unscaled_coef_total_items:.4f}\")\n",
    "print(f\"Scaled Coefficient:   {effect_of_one_std_change:.4f}\\n\")\n",
    "\n",
    "print(f\"Effect of a one-unit increase in '{feature_of_interest}': {effect_of_one_unit_change:.4f} minutes\")\n",
    "print(f\"Effect of a one-standard-deviation increase in '{feature_of_interest}': {effect_of_one_std_change:.4f} minutes\")\n",
    "\n",
    "print(f\"\\nMean of '{feature_of_interest}': {mean_total_items:.4f}\")\n",
    "print(f\"Standard Deviation of '{feature_of_interest}': {std_total_items:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFWJ2s9I_Yeo"
   },
   "source": [
    "Note:\n",
    "The coefficients on the original scale might differ greatly in magnitude from the scaled coefficients, but they both describe the same relationships between variables.\n",
    "\n",
    "Interpretation is key: Focus on the direction and magnitude of the coefficients on the original scale to understand the impact of each variable on the response variable in the original units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClCit1tvKIyE"
   },
   "source": [
    "Include conclusions in your report document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mn-wDgoeSiHP"
   },
   "source": [
    "## Subjective Questions <font color = red>[20 marks]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions only in the notebook. Include the visualisations/methodologies/insights/outcomes from all the above steps in your report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVJSi-Q0Cw_r"
   },
   "source": [
    "#### Subjective Questions based on Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_jiT95xTA6q"
   },
   "source": [
    "##### **Question 1.** <font color = red>[2 marks]</font> <br>\n",
    "\n",
    "Are there any categorical variables in the data? From your analysis of the categorical variables from the dataset, what could you infer about their effect on the dependent variable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TvFQvBy3VM9A"
   },
   "source": [
    "**Answer:**\n",
    "Yes, there are categorical variables in the data. Based on our data preprocessing and exploratory data analysis, the categorical variables are:\n",
    "\n",
    "store_primary_category\n",
    "order_protocol\n",
    "day_of_week\n",
    "hour\n",
    "isWeekend\n",
    "Inferences about their effect on the dependent variable (time_taken):\n",
    "\n",
    "store_primary_category:\n",
    "We used countplot to visualize the distribution of this variable. Different categories of restaurants (e.g., \"fast food,\" \"dine-in\") might have different preparation times, order volumes, and delivery complexities, all of which could affect delivery time. Further analysis (e.g., box plots of time_taken by store_primary_category) would give more insight.\n",
    "order_protocol:\n",
    "Again, countplot showed the distribution. Different order protocols (e.g., orders placed directly through the app vs. orders placed via phone) could influence processing and dispatching efficiency, thus impacting delivery time.\n",
    "day_of_week and hour:\n",
    "We specifically plotted the distribution of time_taken by hour using a boxplot. This visualization showed how delivery times vary throughout the day. We can infer that there are peak hours where delivery times tend to be higher. Similarly, different days of the week may exhibit different order patterns and traffic conditions, influencing delivery times.\n",
    "isWeekend:\n",
    "This is a binary categorical variable we created. Weekends often have higher order volumes and potentially different driver availability, which can affect delivery times.\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KqPxxtWEY3_W"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDSRymTJTHCW"
   },
   "source": [
    "##### **Question 2.** <font color = red>[1 marks]</font> <br>\n",
    "What does `test_size = 0.2` refer to during splitting the data into training and test sets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRBCcZvoVx-r"
   },
   "source": [
    "**Answer:**\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A_afbTV8Y5-F"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEVX57VbTJBP"
   },
   "source": [
    "##### **Question 3.** <font color = red>[1 marks]</font> <br>\n",
    "Looking at the heatmap, which one has the highest correlation with the target variable?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ewPqz4yLWBzR"
   },
   "source": [
    "**Answer:**\n",
    ">\n",
    "test_size = 0.2 means that when splitting the dataset into training and test sets, 20% of the data will be allocated to the test set, and the remaining 80% will be used for the training set. This is a common practice in machine learning to evaluate the model's performance on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLy_-8F5Y69c"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lg-6E-N-TKyS"
   },
   "source": [
    "##### **Question 4.** <font color = red>[2 marks]</font> <br>\n",
    "What was your approach to detect the outliers? How did you address them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wPUDtsRGWLZl"
   },
   "source": [
    "**Answer:**\n",
    ">\n",
    "My approach to detect the outliers involved the following:\n",
    "\n",
    "Visualization using Box Plots: I used box plots to visualize the distribution of the target variable (time_taken) and other numerical features. Box plots are effective in identifying potential outliers as they display the median, quartiles, and the range of the data, with outliers shown as points outside the whiskers.\n",
    "To address the outliers, I applied a combination of techniques:\n",
    "\n",
    "IQR-based Filtering: For some columns, I employed the Interquartile Range (IQR) method to filter out outliers. This involved calculating the first quartile (Q1), third quartile (Q3), and IQR. Data points below Q1 - 1.5 * IQR or above Q3 + 1.5 * IQR were considered outliers and removed.\n",
    "\n",
    "Capping: For some other columns, I used capping to handle outliers. This involved setting upper and lower limits based on percentiles (e.g., 95th percentile) and replacing outlier values with these limits. This method preserves the data points but reduces the impact of extreme values.\n",
    "\n",
    "The specific choice of method (IQR filtering or capping) varied depending on the column and the observed distribution of outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVyJFcT2Y7U8"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dvh9CLFnTMhO"
   },
   "source": [
    "##### **Question 5.** <font color = red>[2 marks]</font> <br>\n",
    "Based on the final model, which are the top 3 features significantly affecting the delivery time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M-DDpZcCWUun"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GVCrLjhTY74h"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VBLH_lA5C4jy"
   },
   "source": [
    "#### General Subjective Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MJGDVyiTOyr"
   },
   "source": [
    "##### **Question 6.** <font color = red>[3 marks]</font> <br>\n",
    "Explain the linear regression algorithm in detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZc1QX8RW_Pa"
   },
   "source": [
    "**Answer:**\n",
    ">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0MCb30NY8UE"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "db_7gqf8TQTk"
   },
   "source": [
    "##### **Question 7.** <font color = red>[2 marks]</font> <br>\n",
    "Explain the difference between simple linear regression and multiple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1jsR8htXD8j"
   },
   "source": [
    "**Answer:**\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FnSGZEltY8ss"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DT6ivEEnTSEs"
   },
   "source": [
    "##### **Question 8.** <font color = red>[2 marks]</font> <br>\n",
    "What is the role of the cost function in linear regression, and how is it minimized?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2PaCL-FXSSn"
   },
   "source": [
    "**Answer:**\n",
    ">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RIKB_W0FY9QM"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gZIb5hbMCCVY"
   },
   "source": [
    "##### **Question 9.** <font color = red>[2 marks]</font> <br>\n",
    "Explain the difference between overfitting and underfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8kn4c-7CEjP"
   },
   "source": [
    "**Answer:**\n",
    "\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8PWIs-suCMEr"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Os7JPKHwArn7"
   },
   "source": [
    "##### **Question 10.** <font color = red>[3 marks]</font> <br>\n",
    "How do residual plots help in diagnosing a linear regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqxU8GSkAubl"
   },
   "source": [
    "**Answer:**\n",
    ">"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "MueJxkvUIII3",
    "02uPO8aQfLnn",
    "b22Kzjew3rdM",
    "u1EBPjFc4Qca",
    "-JJxTsQOFKyl",
    "v0Cd2J-LGWaF",
    "fCLIKw5pQiA7",
    "2bxip-t3Y1MB",
    "mn-wDgoeSiHP"
   ],
   "provenance": [
    {
     "file_id": "1qHefVpjLoVZcdohzmYySNZGiPR4wQOFp",
     "timestamp": 1737728120597
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
